# agri-disease-dataset-pipeline

<p align="center">
  <img src="https://img.shields.io/badge/python-3.10%2B-blue.svg" alt="Python 3.10+">
  <img src="https://img.shields.io/badge/platform-Windows%20%7C%20WSL-success.svg" alt="Platform">
  <img src="https://img.shields.io/badge/tests-pytest-lightgrey.svg" alt="Tests">
  <img src="https://img.shields.io/badge/license-Refer%20dataset%20authors-orange.svg" alt="License">
</p>

> **Reproducible long-path-safe pipeline for extracting, normalizing, and merging PlantVillage, PlantDoc, and Tomato Leaf datasets into a single ML-ready manifest.**

## Highlights

- ğŸ” **End-to-end automation** â€“ unzip, normalize, label, and aggregate datasets in one command.
- ğŸªŸ **Windows-first resilience** â€“ mitigates `MAX_PATH` issues via extended-path copying and rename manifests.
- ğŸ“Š **Consistent outputs** â€“ sequential filenames, CSV labels, metadata JSON, and a unified `combined_dataset.csv`.
- ğŸ§ª **Tested utilities** â€“ label normalization covered by `pytest`; processors structured for extension.

## Repository Layout

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/dataset/            # place plantvillage.zip, plantdoc.zip, tomato leaf zip
â”‚   â””â”€â”€ processed/dataset/      # generated outputs (cleaned on commit)
â”œâ”€â”€ pipeline/                   # core modules (config, fs utils, processors)
â”œâ”€â”€ tests/                      # pytest unit tests
â”œâ”€â”€ docs/                       # architecture & dataset notes (+ diagrams/screenshots)
â”œâ”€â”€ datasets.json               # declarative dataset registry (name, processor, URLs)
â”œâ”€â”€ process_datasets.py         # CLI entry point
â”œâ”€â”€ requirements.txt            # runtime deps
â””â”€â”€ README.md
```

## Quick Links

| Topic | Resource |
|-------|----------|
| Architecture & diagrams | [`docs/pipeline_overview.md`](docs/pipeline_overview.md) |
| Dataset-specific nuances | [`docs/datasets.md`](docs/datasets.md) |
| Issue tracker | GitHub Issues (enable upon publishing) |

## Requirements

- Windows 10/11 with long-paths enabled (or WSL/Ubuntu)
- Python 3.10+
- ~10 GB free disk space for extraction

```bash
python -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt
```

## Dataset Preparation

Copy the raw archives into `data/raw/dataset/` before running the pipeline:

- `plantvillage.zip`
- `plantdoc.zip`
- `Tomato Leaf Dataset  A dataset for multiclass disease detection and classification.zip`

Processed outputs are not tracked; they will be re-created in `data/processed/dataset/` on demand.

## Run the Pipeline

```bash
python process_datasets.py --datasets plantvillage plantdoc tomatoleaf --log-level INFO
```

Add public download URLs via environment variables (`PLANTVILLAGE_URL`, `PLANTDOC_URL`, `TOMATOLEAF_URL`) or per-run overrides:

```bash
python process_datasets.py --download --dataset-url plantdoc=https://example.com/plantdoc.zip
```

Flags:

- `--datasets` â€“ subset to run (`plantvillage`, `plantdoc`, `tomatoleaf`). Defaults to all.
- `--log-level` â€“ logging verbosity (`INFO`, `DEBUG`, ...).
- `--download` â€“ fetch missing archives before processing.
- `--download-only` â€“ fetch archives and exit without extraction.
- `--dataset-url name=url` â€“ per-dataset URL override.

The script clears previous processed folders, extracts long-path-safe copies, and regenerates CSV/metadata files. `combined_dataset.csv` always reflects the datasets processed in the current run.

## Outputs

```
data/processed/dataset/
â”œâ”€â”€ PlantVillage_processed/
â”‚   â”œâ”€â”€ {class_label}/image-00001.jpg
â”‚   â”œâ”€â”€ labels.csv
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ PlantDoc_processed/
â”‚   â”œâ”€â”€ {class_label}/image-00001.jpg
â”‚   â”œâ”€â”€ labels.csv
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ TomatoLeaf_processed/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ annotated/{train,test}/images
â”‚   â”œâ”€â”€ labels/
â”‚   â””â”€â”€ metadata.json
â””â”€â”€ combined_dataset.csv
```

Each extraction also emits `{Dataset}_renamed_files.json` if any filenames were truncated to satisfy Windows path constraints.

## Using Processed Data in ML Pipelines

Load metadata and image paths with the new dataset loader:

```python
from pipeline.dataset_loader import DatasetLoader

loader = DatasetLoader("plantvillage")
print(loader.summary())
df = loader.to_dataframe()
# iterate through resolved image paths
for path in loader.iter_image_paths():
    ...
```

## Testing

```bash
.\.venv\Scripts\python.exe -m pytest tests
```
```bash
python process_datasets.py --download-only --datasets plantdoc --dataset-url plantdoc=https://...
```

## Troubleshooting

| Symptom | Remedy |
|---------|--------|
| `WinError 3` / path too long | Ensure `LongPathsAware=1` or run under WSL; review rename manifest. |
| Missing dataset folders | Confirm zip presence under `data/raw/dataset/`. |
| Antivirus slows extraction | Temporarily exclude the repo path or run on SSD. |
| Need original filenames | Cross-reference `{Dataset}_renamed_files.json` for mappings. |

## Contributing

1. Fork & branch (`git checkout -b feature/...`).
2. Add/adjust processors or utilities under `pipeline/`.
3. Update docs/tests as needed.
4. Run `pytest` + sample `process_datasets.py` invocation.
5. Submit a PR with a concise summary and validation details.

## License / Citation

Refer to the original dataset licenses (PlantVillage, PlantDoc, Tomato Leaf). This pipeline is distributed under the repositoryâ€™s default license; cite the dataset authors in downstream research.
